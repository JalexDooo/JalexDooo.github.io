
@article{an_multidimensional_2025,
	title = {Multidimensional dual encoding network for liver lesion classification from multi-phase magnetic resonance imaging},
	issn = {2948-2933},
	url = {https://link.springer.com/10.1007/s10278-025-01698-x},
	doi = {10.1007/s10278-025-01698-x},
	abstract = {Liver cancer has a high mortality rate and is a serious threat to human life. The study of automated methods for analyzing liver cancer is very helpful to doctors in making a diagnosis. The existing methods tend to ignore the information correlation between multiple modalities of magnetic resonance imaging and do not design networks for multiple modalities and liver lesions. These methods are deficient in liver lesion classification and prediction performance, limiting development of the field. Therefore, we consider the information correlation between the multimodalities and design a multidimensional dual encoding network that can make full use of the information between the eight modalities to improve the classification and the prediction performance of liver lesions. It consists of a multidimensional information extraction, a dual encoder, and a classification structure. Firstly, a method for the application of multimodal data is designed, and the multidimensional information extraction module is used to extract two-dimensional (2D) and three-dimensional (3D) information from all modalities. Then, the dual encoder is used to improve feature extraction and pass multi-scale information to the classification structure. Finally, two differently connected networks were used to train the model for joint prediction, improving the final results. In this paper, a multiphase magnetic resonance imaging dataset containing 498 images was used for the experiments. The method was validated by ablation studies and comparisons with state-of-the-art (SOTA) methods, achieving balanced F1 scores, Cohen\_Kappa, accuracy, and area under curve of 0.781, 0.731, 0.779, and 0.944, respectively.},
	language = {en},
	urldate = {2025-11-12},
	journal = {Journal of Imaging Informatics in Medicine},
	author = {An, Xinjun and Sun, Jindong and Zhang, Yixin and Jiang, Jian and Peng, Yanjun},
	month = oct,
	year = {2025},
	keywords = {/unread, /未读},
	file = {An et al. - 2025 - Multidimensional dual encoding network for liver lesion classification from multi-phase magnetic res.pdf:/Users/jontysun/Zotero/storage/LTP2EVLW/An et al. - 2025 - Multidimensional dual encoding network for liver lesion classification from multi-phase magnetic res.pdf:application/pdf},
}

@article{peng_histogram-driven_2023,
	title = {A histogram-driven generative adversarial network for brain {MRI} to {CT} synthesis},
	volume = {277},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095070512300552X},
	doi = {10.1016/j.knosys.2023.110802},
	abstract = {Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) are commonly used tools for medical diagnostic assessment. Considering the ionizing radiation of CT imaging, estimating CT images from radiation-free MRI would be beneficial for medical diagnosis. Although state-of-art generative adversarial networks or end-to-end architecture models can generate realistic natural images, it is challenging to generate medical CT images with high signal-to-noise ratios and are paired with MRI. We propose a histogram-driven generative adversarial network (HisGAN) to address this issue, estimate CT images paired with MR, and develop a histogram-based dynamic scaling factor to facilitate learning different image styles. By employing an adversarial learning strategy to train the end-toend generator, the generator better simulate the nonlinear mapping from source to target. For the generator, the proposed method applies multiple learnable parameters to adjust the overall weights of the dilated convolution layers to ensure sufficient expansive receptive fields for improved performance. Additionally, the method utilizes deep residual networks to train randomly smoothed generated images and employs adversarial loss to enhance the generation of the discriminator, achieving a balance between the generator and the discriminator. Our approach can synthesize image details at the pixel level in the target domain and has been evaluated using two datasets for MR to CT, T1 to T2, and Flair to T1ce modality synthesis tasks. The proposed method outperforms existing generative adversarial models applied to medical image synthesis.},
	language = {en},
	urldate = {2025-11-12},
	journal = {Knowledge-Based Systems},
	author = {Peng, Yanjun and Sun, Jindong and Ren, Yande and Li, Dapeng and Guo, Yanfei},
	month = oct,
	year = {2023},
	keywords = {/unread, /未读},
	pages = {110802},
	file = {Peng et al. - 2023 - A histogram-driven generative adversarial network for brain MRI to CT synthesis.pdf:/Users/jontysun/Zotero/storage/HFTIFBUZ/Peng et al. - 2023 - A histogram-driven generative adversarial network for brain MRI to CT synthesis.pdf:application/pdf},
}

@article{sun_segmentation_2021,
	title = {Segmentation of the multimodal brain tumor image used the multi-pathway architecture method based on {3D} {FCN}},
	volume = {423},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220315733},
	doi = {10.1016/j.neucom.2020.10.031},
	abstract = {Segmentation of multimodal brain tissues from 3D medical images is of great signiﬁcance for brain diagnosis. It is required to create an automated and accurate segmentation based on deep-learning network due to the manual segmentation is time-consuming. However, how to segment medical images accurately and how to build neural network effectively with very limited computing resource, is still challenging task. To address these problems, we propose a novel model based on 3D fully convolutional network. More speciﬁcally, we apply multi-pathway architecture to feature extraction so as to effectively extract features from multi-modal MRI images. Different receptive ﬁeld of feature have been extracted by adopting 3D dilated convolution in each pathway. By evaluating one-pathway model and key components of our model with a set of effective training schemes, we analyzed how these alternative methods affect the performance of experiments. Our proposed model was evaluated in the Brain Tumor Segmentation 2019 dataset (BraTS 2019), making an effective segmentation for the complete, core and enhancing tumor regions in Dice Similarity Coefﬁcient metric (0.89, 0.78, 0.76) for the dataset. Also, we made a practice on BraTS 2018 using the same method with the Dice Similarity Coefﬁcient metric of 0.90, 0.79, and 0.77 for the complete, core and enhancing tumor regions. Our method is inherently general and is a powerful tool to studies of medical images of brain tumors. Our code is available at https://github.com/JalexDooo/ BrainstormTS.},
	language = {en},
	urldate = {2025-11-12},
	journal = {Neurocomputing},
	author = {Sun, Jindong and Peng, Yanjun and Guo, Yanfei and Li, Dapeng},
	month = jan,
	year = {2021},
	keywords = {/unread, /未读},
	pages = {34--45},
	file = {Sun et al. - 2021 - Segmentation of the multimodal brain tumor image used the multi-pathway architecture method based on.pdf:/Users/jontysun/Zotero/storage/SJXWRTDC/Sun et al. - 2021 - Segmentation of the multimodal brain tumor image used the multi-pathway architecture method based on.pdf:application/pdf},
}

@article{peng_multimodal_2023,
	title = {The multimodal {MRI} brain tumor segmentation based on {AD}-net},
	volume = {80},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S174680942200790X},
	doi = {10.1016/j.bspc.2022.104336},
	abstract = {Multimodal glioma images provide different features of tumor boundaries based on magnetic resonance imaging (MRI), where multimodal features are often challenging to extract for deep learning segmentation methods. Disturbance between features of different modes is an important factor restricting multimodal learning. To efficiently extract multimodal features, we propose an automatic weighted dilated convolutional network (AD-Net) to learn multimodal brain tumor features through channel feature separation learning. Specifically, the auto-weight dilated convolutional unit (AD unit) utilizes dual-scale convolutional feature maps to acquire channel separation features. We employ two learnable parameters to fuse dual-scale convolutional feature maps in encoding layers, and the two learnable parameters are automatically adjusted with the back propagation of the gradient. We adopt the Jensen–Shannon divergence to constrain the distribution of its feature map, which in turn regularizes the weights of the entire down-sampling. In addition, we use the training technique of deep supervision to achieve fast fitting. Our proposed method got dice scores of 0.90, 0.80, and 0.76 for the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET) on the BraTS20 dataset. The experimental results showed good performance with the AD-Net network.},
	language = {en},
	urldate = {2025-11-12},
	journal = {Biomedical Signal Processing and Control},
	author = {Peng, Yanjun and Sun, Jindong},
	month = feb,
	year = {2023},
	keywords = {/unread, /未读},
	pages = {104336},
	file = {Peng and Sun - 2023 - The multimodal MRI brain tumor segmentation based on AD-net.pdf:/Users/jontysun/Zotero/storage/EH9BCBDL/Peng and Sun - 2023 - The multimodal MRI brain tumor segmentation based on AD-net.pdf:application/pdf},
}

@incollection{crimi_segmentation_2021,
	address = {Cham},
	title = {Segmentation of the multimodal brain tumor images used res-{U}-net},
	volume = {12658},
	isbn = {978-3-030-72083-4 978-3-030-72084-1},
	url = {http://link.springer.com/10.1007/978-3-030-72084-1_24},
	abstract = {Gliomas are the most common brain tumors, which have a high mortality. Magnetic resonance imaging (MRI) is useful to assess gliomas, in which segmentation of multimodal brain tissues in 3D medical images is of great signiﬁcance for brain diagnosis. Due to manual job for segmentation is time-consuming, an automated and accurate segmentation method is required. How to segment multimodal brain accurately is still a challenging task. To address this problem, we employ residual neural blocks and a U-Net architecture to build a novel network. We have evaluated the performances of diﬀerent primary residual neural blocks in building U-Net. Our proposed method was evaluated on the validation set of BraTS 2020, in which our model makes an eﬀective segmentation for the complete, core and enhancing tumor regions in Dice Similarity Coeﬃcient (DSC) metric (0.89, 0.78, 0.72). And in testing set, our model got the DSC results of 0.87, 0.82, 0.80. Residual convolutional block is especially useful to improve performance in building model. Our proposed method is inherently general and is a powerful tool to studies of medical images of brain tumors.},
	language = {en},
	urldate = {2025-11-12},
	booktitle = {Brainlesion: {Glioma}, {Multiple} {Sclerosis}, {Stroke} and {Traumatic} {Brain} {Injuries}},
	publisher = {Springer International Publishing},
	author = {Sun, Jindong and Peng, Yanjun and Li, Dapeng and Guo, Yanfei},
	editor = {Crimi, Alessandro and Bakas, Spyridon},
	year = {2021},
	doi = {10.1007/978-3-030-72084-1_24},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {/unread, /已读},
	pages = {263--273},
	file = {Sun et al. - 2021 - Segmentation of the multimodal brain tumor images used res-U-net.pdf:/Users/jontysun/Zotero/storage/AUYYZG8Y/Sun et al. - 2021 - Segmentation of the multimodal brain tumor images used res-U-net.pdf:application/pdf},
}

@article{li_task-unified_2023,
	title = {A task-unified network with transformer and spatial–temporal convolution for left ventricular quantification},
	volume = {13},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-40841-y},
	doi = {10.1038/s41598-023-40841-y},
	abstract = {Abstract
            Quantification of the cardiac function is vital for diagnosing and curing the cardiovascular diseases. Left ventricular function measurement is the most commonly used measure to evaluate the function of cardiac in clinical practice, how to improve the accuracy of left ventricular quantitative assessment results has always been the subject of research by medical researchers. Although considerable efforts have been put forward to measure the left ventricle (LV) automatically using deep learning methods, the accurate quantification is yet a challenge work as a result of the changeable anatomy structure of heart in the systolic diastolic cycle. Besides, most methods used direct regression method which lacks of visual based analysis. In this work, a deep learning segmentation and regression task-unified network with transformer and spatial–temporal convolution is proposed to segment and quantify the LV simultaneously. The segmentation module leverages a U-Net like 3D Transformer model to predict the contour of three anatomy structures, while the regression module learns spatial–temporal representations from the original images and the reconstruct feature map from segmentation path to estimate the finally desired quantification metrics. Furthermore, we employ a joint task loss function to train the two module networks. Our framework is evaluated on the MICCAI 2017 Left Ventricle Full Quantification Challenge dataset. The results of experiments demonstrate the effectiveness of our framework, which achieves competitive cardiac quantification metric results and at the same time produces visualized segmentation results that are conducive to later analysis.},
	language = {en},
	number = {1},
	urldate = {2025-11-12},
	journal = {Scientific Reports},
	author = {Li, Dapeng and Peng, Yanjun and Sun, Jindong and Guo, Yanfei},
	month = aug,
	year = {2023},
	keywords = {/unread, /未读},
	pages = {13529},
	file = {Li et al. - 2023 - A task-unified network with transformer and spatial–temporal convolution for left ventricular quanti.pdf:/Users/jontysun/Zotero/storage/SFAW2DIW/Li et al. - 2023 - A task-unified network with transformer and spatial–temporal convolution for left ventricular quanti.pdf:application/pdf},
}

@article{chen_dmca-gan_2023,
	title = {{DMCA}-{GAN}: dual multilevel constrained attention {GAN} for {MRI}-based hippocampus segmentation},
	volume = {36},
	issn = {0897-1889, 1618-727X},
	shorttitle = {Dmca-gan},
	url = {https://link.springer.com/10.1007/s10278-023-00854-5},
	doi = {10.1007/s10278-023-00854-5},
	abstract = {Precise segmentation of the hippocampus is essential for various human brain activity and neurological disorder studies. To overcome the small size of the hippocampus and the low contrast of MR images, a dual multilevel constrained attention GAN for MRI-based hippocampus segmentation is proposed in this paper, which is used to provide a relatively effective balance between suppressing noise interference and enhancing feature learning. First, we design the dual-GAN backbone to effectively compensate for the spatial information damage caused by multiple pooling operations in the feature generation stage. Specifically, dual-GAN performs joint adversarial learning on the multiscale feature maps at the end of the generator, which yields an average Dice coefficient (DSC) gain of 5.95\% over the baseline. Next, to suppress MRI high-frequency noise interference, a multilayer information constraint unit is introduced before feature decoding, which improves the sensitivity of the decoder to forecast features by 5.39\% and effectively alleviates the network overfitting problem. Then, to refine the boundary segmentation effects, we construct a multiscale feature attention restraint mechanism, which forces the network to concentrate more on effective multiscale details, thus improving the robustness. Furthermore, the dual discriminators D1 and D2 also effectively prevent the negative migration phenomenon. The proposed DMCA-GAN obtained a DSC of 90.53\% on the Medical Segmentation Decathlon (MSD) dataset with tenfold cross-validation, which is superior to the backbone by 3.78\%.},
	language = {en},
	number = {6},
	urldate = {2025-11-12},
	journal = {Journal of Digital Imaging},
	author = {Chen, Xue and Peng, Yanjun and Li, Dapeng and Sun, Jindong},
	month = dec,
	year = {2023},
	keywords = {/unread, /未读},
	pages = {2532--2553},
	file = {Chen et al. - 2023 - DMCA-GAN dual multilevel constrained attention GAN for MRI-based hippocampus segmentation.pdf:/Users/jontysun/Zotero/storage/BQBDIRKH/Chen et al. - 2023 - DMCA-GAN dual multilevel constrained attention GAN for MRI-based hippocampus segmentation.pdf:application/pdf},
}

@article{guo_dsln_2022,
	title = {{DSLN}: dual-tutor student learning network for multiracial glaucoma detection},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	shorttitle = {Dsln},
	url = {https://link.springer.com/10.1007/s00521-022-07078-8},
	doi = {10.1007/s00521-022-07078-8},
	abstract = {Accurate early glaucoma detection is crucial to prevent further vision loss. However, using the off-the-shelf models against fundus image datasets of different races may lead to degraded performance due to domain shift. To address the issue, this paper proposes a dual-tutor student learning network (DSLN) for multiracial glaucoma detection. The proposed DSLN consists of an inter-image tutor, an intra-image tutor, student model and backbone network, which combines the advantages of domain adaptation and semi-supervised learning. The inter-image tutor uses CycleGAN for style transfer to reduce the appearance differences between labeled source domain and labeled target domain images, and transfers the learned knowledge to the student model by minimizing knowledge distillation loss. The intra-image tutor adopts the exponential moving average to leverage the unlabeled target domain and transfers the knowledge to the student model by minimizing prediction consistency loss. Moreover, the student model not only directly learns knowledge from the labeled target domain images, but also learns the intra-image knowledge and inter-image knowledge transfer by two tutors. Furthermore, the backbone integrates the context features of the local optic disc region and global fundus image via modiﬁed ResNet50. We conduct extensive experiments on three scenarios constructed from nine public fundus image datasets of three races. Comprehensive experimental results show that the proposed DSLN framework outperforms the state-of-the-art models and has good robustness and generalization: it can effectively overcome domain shift and accurately detect glaucoma from multi-ethnic fundus images.},
	language = {en},
	number = {14},
	urldate = {2025-11-12},
	journal = {Neural Computing and Applications},
	author = {Guo, Yanfei and Peng, Yanjun and Sun, Jindong and Li, Dapeng and Zhang, Bin},
	month = jul,
	year = {2022},
	keywords = {/unread, /未读},
	pages = {11885--11910},
	file = {Guo et al. - 2022 - DSLN dual-tutor student learning network for multiracial glaucoma detection.pdf:/Users/jontysun/Zotero/storage/P8DVNWYK/Guo et al. - 2022 - DSLN dual-tutor student learning network for multiracial glaucoma detection.pdf:application/pdf},
}

@article{chen_mlrd-net_2022,
	title = {{MLRD}-net: {3D} multiscale local cross-channel residual denoising network for {MRI}-based brain tumor segmentation},
	volume = {60},
	issn = {0140-0118, 1741-0444},
	shorttitle = {{MLRD}-net},
	url = {https://link.springer.com/10.1007/s11517-022-02673-2},
	doi = {10.1007/s11517-022-02673-2},
	abstract = {The precise segmentation of multimodal MRI images is the primary stage of tumor diagnosis and treatment. Current segmentation strategies often underutilize multiscale features, which can easily lead to loss of contextual information, reduction of low-level features and noise interference. To overcome these issues, a 3D multiscale local cross-channel residual denoising network (MLRD-Net) for an MRI-based brain tumor segmentation algorithm is proposed in this paper. Specifically, we employ encoder-decoder structure to connect local and global features, and enhance the receptive field of the network. Random slice operation has been conducted to enhance robustness. Then, residual blocks with pre-activation operation are developed in down-sampling stage, which effectively improves signal propagation along the network and alleviates network overfitting. Finally, the local cross-channel denoising mechanism is established to eliminate unimportant features without dimensionality reduction. Our proposal was evaluated in Brain Tumor Segmentation 2020 dataset (BraTS 2020), obtaining significantly improved results with mean Dice Similarity Coefficient metric of 0.91, 0.79, and 0.73 for the complete, tumor core, and enhancing tumor regions respectively. Besides, we conduct further practice on BraTS 2019, with the mean Dice Similarity Coefficient metric of 0.89, 0.80, and 0.75. Massive experiments demonstrate that our method is powerful and reliable. It increases little model complexity while achieving very competitive performance.},
	language = {en},
	number = {12},
	urldate = {2025-11-12},
	journal = {Medical and Biological Engineering and Computing},
	author = {Chen, Xue and Peng, Yanjun and Guo, Yanfei and Sun, Jindong and Li, Dapeng and Cui, Jianming},
	month = dec,
	year = {2022},
	keywords = {/unread, /未读},
	pages = {3377--3395},
	file = {Chen et al. - 2022 - MLRD-net 3D multiscale local cross-channel residual denoising network for MRI-based brain tumor seg.pdf:/Users/jontysun/Zotero/storage/2JINXQAA/Chen et al. - 2022 - MLRD-net 3D multiscale local cross-channel residual denoising network for MRI-based brain tumor seg.pdf:application/pdf},
}

@article{li_taunet_2022,
	title = {{TAUNet}: a triple-attention-based multi-modality {MRI} fusion {U}-net for cardiac pathology segmentation},
	volume = {8},
	issn = {2199-4536, 2198-6053},
	shorttitle = {{TAUNet}},
	url = {https://link.springer.com/10.1007/s40747-022-00660-6},
	doi = {10.1007/s40747-022-00660-6},
	abstract = {Automated segmentation of cardiac pathology in MRI plays a signiﬁcant role for diagnosis and treatment of some cardiac disease. In clinical practice, multi-modality MRI is widely used to improve the cardiac pathology segmentation, because it can provide multiple or complementary information. Recently, deep learning methods have presented implausible performance in multi-modality medical image segmentation. However, how to fuse the underlying multi-modality information effectively to segment the pathology with irregular shapes and small region at random locations, is still a challenge task. In this paper, a triple-attention-based multi-modality MRI fusion U-Net was proposed to learn complex relationship between different modalities and pay more attention on shape information, thus to achieve improved pathology segmentation. First, three independent encoders and one fusion encoder were applied to extract speciﬁc and multiple modality features. Secondly, we concatenate the modality feature maps and use the channel attention to fuse speciﬁc modal information at every stage of the three dedicate independent encoders, then the three single modality feature maps and channel attention feature maps are together concatenated to the decoder path. Spatial attention was adopted in decoder path to capture the correlation of various positions. Once more, we employ shape attention to focus shape-dependent information. Lastly, the training approach is made efﬁcient by introducing deep supervision mechanism with object contextual representations block to ensure precisely boundary prediction. Our proposed network was evaluated on the public MICCAI 2020 Myocardial pathology segmentation dataset which involves patients suffering from myocardial infarction. Experiments on the dataset with three modalities demonstrate the effectiveness of fusion mode of our model, and attention mechanism can integrate various modality information well. We demonstrated that such a deep learning approach could better fuse complementary information to improve the segmentation performance of cardiac pathology.},
	language = {en},
	number = {3},
	urldate = {2025-11-12},
	journal = {Complex \& Intelligent Systems},
	author = {Li, Dapeng and Peng, Yanjun and Guo, Yanfei and Sun, Jindong},
	month = jun,
	year = {2022},
	keywords = {/unread, /未读},
	pages = {2489--2505},
	file = {Li et al. - 2022 - TAUNet a triple-attention-based multi-modality MRI fusion U-net for cardiac pathology segmentation.pdf:/Users/jontysun/Zotero/storage/PMEFXK9A/Li et al. - 2022 - TAUNet a triple-attention-based multi-modality MRI fusion U-net for cardiac pathology segmentation.pdf:application/pdf},
}

@article{li_unsupervised_2023,
	title = {Unsupervised deep consistency learning adaptation network for cardiac cross-modality structural segmentation},
	volume = {61},
	issn = {0140-0118, 1741-0444},
	url = {https://link.springer.com/10.1007/s11517-023-02833-y},
	doi = {10.1007/s11517-023-02833-y},
	abstract = {Deep neural networks have recently been succeessful in the ﬁeld of medical image segmentation; however, they are typically subject to performance degradation problems when well-trained models are tested in another new domain with different data distributions. Given that annotated cross-domain images may inaccessible, unsupervised domain adaptation methods that transfer learnable information from annotated source domains to unannotated target domains with different distributions have attracted substantial attention. Many methods leverage image-level or pixel-level translation networks to align domaininvariant information and mitigate domain shift issues. However, These methods rarely perform well when there is a large domain gap. A new unsupervised deep consistency learning adaptation network, which adopts input space consistency learning and output space consistency learning to realize unsupervised domain adaptation and cardiac structural segmentation, is introduced in this paper The framework mainly includes a domain translation path and a cross-modality segmentation path. In domain translation path, a symmetric alignment generator network with attention to cross-modality features and anatomy is introduced to align bidirectional domain features. In the segmentation path, entropy map minimization, output probability map minimization and segmentation prediction minimization are leveraged to align the output space features. The model conducts supervised learning to extract source domain features and conducts unsupervised deep consistency learning to extract target domain features. Through experimental testing on two challenging cross-modality segmentation tasks, our method has robust performance compared to that of previous methods. Furthermore, ablation experiments are conducted to conﬁrm the effectiveness of our framework.},
	language = {en},
	number = {10},
	urldate = {2025-11-12},
	journal = {Medical and Biological Engineering and Computing},
	author = {Li, Dapeng and Peng, Yanjun and Sun, Jindong and Guo, Yanfei},
	month = oct,
	year = {2023},
	keywords = {/unread, /未读},
	pages = {2713--2732},
	file = {Li et al. - 2023 - Unsupervised deep consistency learning adaptation network for cardiac cross-modality structural segm.pdf:/Users/jontysun/Zotero/storage/LZWAIGEH/Li et al. - 2023 - Unsupervised deep consistency learning adaptation network for cardiac cross-modality structural segm.pdf:application/pdf},
}

@article{sun_cross-modality_2024,
	title = {The cross-modality survival prediction method of glioblastoma based on dual-graph neural networks},
	volume = {254},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417424012600},
	doi = {10.1016/j.eswa.2024.124394},
	abstract = {Glioma, the highly lethal malignant brain tumor originating from abnormal proliferation of glial cells, exhibits a varied overall survival rate influenced by multiple factors. Accurate prediction rate of survival periods assist physicians in selecting the most suitable treatment plans to improve patient overall survival (OS) rates. The paper proposes a dual-graph neural network (GNN) with manually constructed feature relational graph for OS prediction and inference of different survival periods in glioblastoma based on cross-modality data. Specifically, five radiomic features from magnetic resonance imaging are extracted to construct two sets of feature relational graphs. The main GNN is utilized to extract comprehensive features, including age, brain MRI features, and radiomics features of gliomas. The branch GNN additionally extracts radiomics features specific to gliomas, constraining the feature weights of the main GNN through attention mechanisms. Pretraining an autoencoder to extract deep features from patient text information. The text features and image features are then reorganized based on features from different modalities through a transformer decoder. Finally, a multi-layer perceptron is utilized for regression and classification, thus enabling the classification and prediction of patient survival. The proposed method achieved an accuracy of 0.586 for classifying and predicting the survival of glioma patients in the short, medium, and long term on the BraTS20 dataset, outperforming state-of-the-art methods.},
	language = {en},
	urldate = {2025-11-12},
	journal = {Expert Systems with Applications},
	author = {Sun, Jindong and Peng, Yanjun},
	month = nov,
	year = {2024},
	keywords = {/unread, /未读},
	pages = {124394},
	file = {Sun and Peng - 2024 - The cross-modality survival prediction method of glioblastoma based on dual-graph neural networks.pdf:/Users/jontysun/Zotero/storage/KIF4KKZ2/Sun and Peng - 2024 - The cross-modality survival prediction method of glioblastoma based on dual-graph neural networks.pdf:application/pdf},
}

@article{li_mfaunet_2022,
	title = {{MFAUNet}: multiscale feature attentive {U}‐net for cardiac {MRI} structural segmentation},
	volume = {16},
	issn = {1751-9659, 1751-9667},
	shorttitle = {{MFAUNet}},
	url = {https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/ipr2.12406},
	doi = {10.1049/ipr2.12406},
	abstract = {The accurate and robust automatic segmentation of cardiac structures in magnetic resonance imaging (MRI) is signiﬁcant in calculating cardiac clinical functional indices, and diagnosing heart diseases. Most U-Net based methods use pooling, transposed convolution, and skip connection operations to integrate the multiscale features for improved segmentation in cardiac MRI. However, this architecture lacks adequate semantic connection between the channel and spatial information, and robustness in segmenting objects with signiﬁcant shape variations. In this paper, a new multiscale feature attentive U-Net for cardiac MRI structural segmentation method is proposed. An attention mechanism is adopted after concatenating the multi-level features to aggregate different scale features and determine on which features to focus. Cascade and parallel dilated convolution is also employed in the decoder blocks and skip connection is employed to enhance the ability of sensing receptive ﬁelds for multiscale context information. Furthermore, deep supervision approach with a loss function that combines the dice and cross-entropy losses to reduce overﬁtting and ensure better prediction is introduced. The proposed method was evaluated on three public cardiac datasets. The experimental results indicate that the method achieved competitive segmentation performance with the three datasets, which veriﬁes the robustness and generalisability of the proposed network. In comparison with conventional U-Net methods, the model leverages attention mechanism and dilated convolution block, which increases the semantic connection between the channel and the spatial information, and improves the robustness of the right ventricle segmentation performance. From the view of the Dice scores and segmentation results, the multiscale feature attentive U-Net method is one of effective methods in segmenting cardiac MRI structures.},
	language = {en},
	number = {4},
	urldate = {2025-11-12},
	journal = {IET Image Processing},
	author = {Li, Dapeng and Peng, Yanjun and Guo, Yanfei and Sun, Jindong},
	month = mar,
	year = {2022},
	keywords = {/unread, /未读},
	pages = {1227--1242},
	file = {Li et al. - 2022 - MFAUNet multiscale feature attentive U‐net for cardiac MRI structural segmentation.pdf:/Users/jontysun/Zotero/storage/RQIV7T73/Li et al. - 2022 - MFAUNet multiscale feature attentive U‐net for cardiac MRI structural segmentation.pdf:application/pdf},
}

@incollection{andrearczyk_ccut-net_2022,
	address = {Cham},
	title = {{CCUT}-{Net}: {Pixel}-{Wise} {Global} {Context} {Channel} {Attention} {UT}-{Net} for {Head} and {Neck} {Tumor} {Segmentation}},
	volume = {13209},
	isbn = {978-3-030-98252-2 978-3-030-98253-9},
	shorttitle = {{CCUT}-{Net}},
	url = {https://link.springer.com/10.1007/978-3-030-98253-9_2},
	language = {en},
	urldate = {2025-11-12},
	booktitle = {Head and {Neck} {Tumor} {Segmentation} and {Outcome} {Prediction}},
	publisher = {Springer International Publishing},
	author = {Wang, Jiao and Peng, Yanjun and Guo, Yanfei and Li, Dapeng and Sun, Jindong},
	editor = {Andrearczyk, Vincent and Oreiller, Valentin and Hatt, Mathieu and Depeursinge, Adrien},
	year = {2022},
	doi = {10.1007/978-3-030-98253-9_2},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {/unread, /未读},
	pages = {38--49},
	file = {Wang et al. - 2022 - CCUT-Net Pixel-Wise Global Context Channel Attention UT-Net for Head and Neck Tumor Segmentation.pdf:/Users/jontysun/Zotero/storage/4QDRUKUY/Wang et al. - 2022 - CCUT-Net Pixel-Wise Global Context Channel Attention UT-Net for Head and Neck Tumor Segmentation.pdf:application/pdf},
}
